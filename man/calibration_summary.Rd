% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/calibration_summary.R
\name{calibration_summary}
\alias{calibration_summary}
\title{Compute a Summary of Meta-Analytic Bayes Factors and Effect Sizes}
\usage{
calibration_summary(
  data,
  dataset_col = NULL,
  measurement_name_col,
  measurement_values_col,
  standard_scores_col,
  subject_id_col = NULL,
  attenuation_factor = 0.9,
  power = 0.8,
  sig.level = 0.05,
  alternative = "two.sided"
)
}
\arguments{
\item{data}{A dataframe containing multiple datasets, measurement types, and measurement values.}

\item{dataset_col}{A string specifying the column name that identifies datasets - if NULL, all data pertains to one dataset.}

\item{measurement_name_col}{A string specifying the column name that identifies different measurements.}

\item{measurement_values_col}{A string specifying the column name containing measurement values.}

\item{standard_scores_col}{A string specifying the column name containing standard scores.}

\item{subject_id_col}{A string specifying the column name containing subject IDs - can be NULL.}

\item{attenuation_factor}{A numeric value (0 < x ≤ 1) indicating the expected proportional transfer
of the calibration effect to the target context. For example, \code{attenuation_factor = 0.9}
assumes real-world effects are expected to be 90\% as strong as those observed in calibration.
Only applicable to binary designs.}

\item{power}{Power of test (1 minus Type II error probability); used to compute sample size. See pwr::pwr.t.test.}

\item{sig.level}{Significance level (Type I error probability); used to compute sample size. See pwr::pwr.t.test.}

\item{alternative}{a character string specifying the alternative hypothesis, must be one of "two.sided" (default),
"greater" or "less"; used to compute sample size. See pwr::pwr.t.test.}
}
\value{
A list containing:
\describe{
\item{summary_table}{A dataframe summarizing results from Bayes Factor and effect size computations.}
\item{log_bayes_factor_diffs}{A matrix of Bayes Factor log-ratio differences between all measurements.}
\item{best_measurement}{A descriptive summary of the best-performing measurement.}
\item{subject_overlap}{A matrix showing the percentage of shared subjects between measurement types,
based on combined dataset-subject identifiers. Returned only if subject_id_col is provided and multiple datasets exist.}
}
}
\description{
This function computes a calibration summary by:
\itemize{
\item Running \code{meta_bf_from_data()} for \strong{Bayes Factors}.
\item Running \code{meta_cohensd()} for \strong{Cohen's d and Hedges' g} (only if \code{standard_scores_col} is binary).
\item Returning a \strong{matrix of Bayes Factor ratios} between measurements.
\item Identifying \strong{the best measurement} based on Bayes Factor.
\item Optionally returning the \strong{subject overlap matrix} (percentage of shared data between measurements).
}
}
\details{
\itemize{
\item If \code{standard_scores_col} is \strong{binary (0/1)}, effect sizes (Cohen's d, Hedges' g) will be computed.
\item If \code{standard_scores_col} is \strong{continuous}, only Bayes Factors will be computed.
\item The \code{attenuation_factor} rescales the inferred meta-analytic Cohen’s d to reflect the expected
proportion of the calibration effect that generalizes to a new or target context.
For example, setting \code{attenuation_factor = 0.8} means that only 80\% of the calibration effect
is assumed to hold in the substantive experiment, leading to larger required sample sizes.
}
}
\examples{
require(dplyr)
set.seed(123)

generate_study <- function(study_id, n_per_group, within = TRUE) {
  df <- expand.grid(
    study_id = study_id,
    subject_id = 1:n_per_group,
    standard_score = c(0, 1),
    measurement_type = c("A", "B")
  )
  if (!within) df$subject_id <- 1:nrow(df)
  df$measurement <- with(df, rnorm(nrow(df),
    mean = ifelse(measurement_type == "A",
                  ifelse(standard_score == 1, 10, 5),
                  ifelse(standard_score == 1, 7, 6)),
    sd = 5))
  return(df)
}

all_studies <- do.call(rbind, list(
  generate_study("Study1", 30, within = TRUE),
  generate_study("Study2", 30, within = TRUE),
  generate_study("Study3", 30, within = FALSE)
))

result <- calibration_summary(
  data = all_studies,
  dataset_col = "study_id",
  standard_scores_col = "standard_score",
  measurement_name_col = "measurement_type",
  measurement_values_col = "measurement",
  subject_id_col = "subject_id",
  attenuation_factor = 0.9
)

print(result$summary_table)
print(result$log_bayes_factor_diffs)
print(result$subject_overlap)
print(result$best_measurement)

}
